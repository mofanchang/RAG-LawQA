import pickle
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# === è¼‰å…¥è³‡æ–™èˆ‡æ¨¡å‹ ===
with open("laws_clean.pkl", "rb") as f:
    cleaned_articles = pickle.load(f)

index = faiss.read_index("law_embeddings_ip.faiss")

embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype="float16")
selected_model = ""
tokenizer = AutoTokenizer.from_pretrained(selected_model, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    selected_model, quantization_config=bnb_config, device_map="auto",
    torch_dtype="auto", trust_remote_code=True, low_cpu_mem_usage=True
)

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=256, temperature=0.3, do_sample=True)
llm = HuggingFacePipeline(pipeline=pipe)
chat_llm = ChatHuggingFace(llm=llm)

# === æç¤ºè© ===
ad_prompt = PromptTemplate.from_template("""ä½ æ˜¯å°ç£é£Ÿå“è—¥ç‰©ç®¡ç†ç½²çš„è³‡æ·±æ³•è¦å°ˆå®¶ã€‚è«‹åš´æ ¼ä¾æ“šæ³•æ¢åˆ¤æ–·å»£å‘Šè©æ˜¯å¦é•æ³•ã€‚

### ç›¸é—œæ³•æ¢ï¼š
{context}

### å»£å‘Šè©ï¼š
"{ad_text}"

### åˆ¤å®šçµæœï¼š
ã€é•æ³•åˆ¤å®šã€‘ï¼šâŒ é•æ³• / âœ… åˆæ³•
ã€é•æ³•ç†ç”±ã€‘ï¼š
- é•æ³•æ¢æ–‡
- é—œéµè©
- æ³•å¾‹é¢¨éšª
ã€ä¿®æ”¹å»ºè­°ã€‘ï¼š
åˆæ³•æ›¿ä»£ç”¨è©
""")

law_prompt = PromptTemplate.from_template("""ä½ æ˜¯å°ç£é£Ÿè—¥ç½²çš„æ³•å‹™é¡§å•ï¼Œè«‹æä¾›æ³•è¦è§£é‡‹ã€‚

### ç›¸é—œæ³•æ¢ï¼š
{context}

### å•é¡Œï¼š
{question}

### å›æ‡‰æ ¼å¼ï¼š
ã€æ³•æ¢ä¾æ“šã€‘ï¼šå…·é«”æ¢æ–‡
ã€è§£é‡‹èªªæ˜ã€‘ï¼šè©³ç´°èªªæ˜
ã€åŸ·æ³•æ¨™æº–ã€‘ï¼šåŸ·è¡Œä¾æ“š
ã€é•æ³•å¾Œæœã€‘ï¼šè™•ç½°å…§å®¹
ã€åˆè¦å»ºè­°ã€‘ï¼šæ”¹å–„å»ºè­°
""")

# === æŸ¥è©¢èˆ‡æ¨è«–å‡½æ•¸ ===
def semantic_search(query, top_k=3):
    vec = embedding_model.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(vec)
    scores, indices = index.search(vec, top_k)
    return [cleaned_articles[i] for i in indices[0] if i != -1]

def test_ad_detection(ad_text):
    context = "\n".join(semantic_search(ad_text))
    chain = ad_prompt | chat_llm | StrOutputParser()
    result = chain.invoke({"context": context, "ad_text": ad_text})
    print(f"\nğŸ“¢ å»£å‘Šï¼š{ad_text}\n{result}")

def test_law_query(question):
    context = "\n".join(semantic_search(question))
    chain = law_prompt | chat_llm | StrOutputParser()
    result = chain.invoke({"context": context, "question": question})
    print(f"\nâ“ å•é¡Œï¼š{question}\n{result}")

# === ç¯„ä¾‹åŸ·è¡Œ ===
if __name__ == "__main__":
    test_ad_detection("æœ¬ç”¢å“èƒ½æœ‰æ•ˆé é˜²ç™Œç—‡ï¼")
    test_law_query("é£Ÿå“å»£å‘Šä¸èƒ½ä½¿ç”¨å“ªäº›è©å½™ï¼Ÿ")

